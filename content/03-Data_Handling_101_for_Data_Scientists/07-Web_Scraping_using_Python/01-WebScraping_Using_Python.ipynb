{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e5ce627a4d994d7c972ccb773f8b4949"
   },
   "source": [
    "# Web Scraping using Python\n",
    "\n",
    "## 1.1 What is Web Scraping\n",
    "\n",
    "In any given field, there is a growing amount of data collected and presented all over the web. Some of your projects might need access to the data presented at various websites. For example, you might want to collect the stock prices information presented at Bloomberg or Google Finance website. You could casually browse these websites to get the information you are looking for. How  do you allow your program to structurally read these websites and consume the information the applications need? Web Scraping helps you in retrieving such information the program needs in a meaningful way.\n",
    "\n",
    "In order to create such Web Scraping programs, we first need to understand the structure of a typical web page and/or web site. We will explain the details in the next section. In addition to understanding what you can expect in a web page, you need to tools to extract information from such web pages. These could be libraries exposed to you in any programming language. In python, there are several such libraries, one of which is Beautiful Soup. In the subsequent sections we will describe how to use such a library in your program.   \n",
    "\n",
    "\n",
    "## 1.2 Structure of a Web Page\n",
    "\n",
    "A Website is really what the designer of the website thinks, an abstraction. The website is a collection of individual pages. It can change from website to website and the type of website, an aplication or a static page. However, over the years as the web technology matured, there are some generic layers to a webpage that you come to expect. A webpage typically contains:\n",
    "\n",
    "Header\n",
    "Navigation\n",
    "Bread Crumb\n",
    "Tab Navigation\n",
    "Content Pane\n",
    "    Left Navigation Links\n",
    "    Main content\n",
    "    Right Details\n",
    "Footer\n",
    "\n",
    "In most cases, we are interested in the content pane to extract the most useful information from the websites.\n",
    "\n",
    "\n",
    "## 1.3 Http Request\n",
    "\n",
    "Web pages are accessed using http requests. Python provides a module called \"requests\".\n",
    "The requests module allows a python program to read a web page by taking in a URL string.\n",
    "\n",
    "\n",
    "\n",
    "``` python\n",
    "import requests\n",
    "response = requests.get(\"http://www.google.com\")\n",
    "\n",
    "```\n",
    "\n",
    "As seen above, import imports the requests object.The get method takes a URL string and gets the html page from the website referred by the URL.\n",
    "\n",
    "In the followin code example, we will see a code snippet that reads a page and prints out the html text.\n",
    "In this example, we will use a web site called http://quotes.toscrape.com/ which is specifically designed as a play ground for web scraping. This site has a list of nice quotes by famous authors. The quotes are presented in the web page as tables and div's. The idea is to read the quotes and the corresponding authors from the page. In addition, we could extract additional information about the author, such as the authors' date of birth. This information is provided in a separate author's page.\n",
    "\n",
    "In the following sections, we will read all of these information for each quote and create our own quotes dataset.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write code to open url \"http://quotes.toscrape.com/\" and print the html response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "c17a2c32913d447db182ece3f6f02476"
   },
   "outputs": [],
   "source": [
    "#write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "20c219c661cd4f129e1d2956e47dbf24",
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "print(html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "d6aa130b0bd7420b9109d32f4beeb017"
   },
   "source": [
    "## 2. Parsing Website Content using Beautiful Soup\n",
    "\n",
    "There are several libraries available in Python for the purpose of web scraping such as Beautifu Soup, Scrapy etc. In this lesson we will see how to use Beautiful Soup to scrape a web site.\n",
    "\n",
    "### 2.1 Beautiful Soup\n",
    "\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. The main class is BeautifulSoup which is part of a library bs4 (BeautifulSoup 4.0). In order to start using BeautifulSoup, we need to import BeautifulSoup from bs4.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "\n",
    "\n",
    "### 2.2 Parsing the web pages \n",
    "\n",
    "In order to start using the module for parsing web pages, you start by passing the html document to BeautifulSoup.\n",
    "For example:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup('<a href=\"http://www.google.com\">Click to visit google.com</b>')\n",
    "```\n",
    "\n",
    "\n",
    "### 2.3 Reading the content usign tags and classes\n",
    "\n",
    "We can read a file line-by-line using a **for loop**. This is both efficient and fast.\n",
    "\n",
    "```python\n",
    "a_tag = soup.a\n",
    "print(a_tag)\n",
    "\n",
    "#For a specific class you can use find_all() with class_ attribute as below\n",
    "specific_tags = soup.find_all(\"div\", class_=\"my_class\")\n",
    "\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Write code to open \"http://quotes.toscrape.com/\" using BeautifulSoup. Read and print all html tags (div) with class as \"quote\". Save it in variable 'tag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "7e362d52a8994ad2927605a3c0ba8b9c"
   },
   "outputs": [],
   "source": [
    "#Write your code below\n",
    "#hint use requests.get() and the url to get the html content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0829bb5052dc48fe93752d982f34f5a2",
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'http://quotes.toscrape.com/'\n",
    "response = requests.get(url)\n",
    "html = response.content\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "tag = soup.find_all(\"div\", class_=\"quote\")\n",
    "print(tag)\n",
    "```\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "39ceded974dd47e086fdcdbf25391345"
   },
   "source": [
    "## 3. Getting to the detailed tags in HTML\n",
    "\n",
    "Once we find a way to parse the html, the next step is to get to the individual tags that we are interested in.\n",
    "\n",
    "### 2.1 Class and Tags\n",
    "\n",
    "Beautiful Soup provides functions to obtain specific tags and search for tags and class.\n",
    "\n",
    "```python\n",
    "tag = html_text.find(\"div\", class_=\"myclass\")\n",
    "```\n",
    "\n",
    "In the above example, we can find the specific tag , div in this case, where the class is \"myclass\". This fetches the first such result. There is also a find_all function that provies a list of such tags as a result of the search.\n",
    "\n",
    "### 2.2 Getting the content within\n",
    "\n",
    "In order to fetch the content, use tag.text.\n",
    "For example, in this case the code will be similar to below:\n",
    "\n",
    "```python\n",
    "    content = tag.text\n",
    "    print(\"Content is :: \", content)\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the previous exercise 'tag' variable with all DIV with class \"quote\", find all \"small\" tags with the class as  \"author\". Print the content within the resulting div tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "47fef65256bd4e1b984e801b7e6b5db1"
   },
   "outputs": [],
   "source": [
    "#write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "ca262a997b6541439bad9d40459af48f",
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "for t in tag:\n",
    "    print(t.span.text)\n",
    "    a = t.find(\"small\", class_=\"author\")\n",
    "    author = a.text\n",
    "    print(\"By :: \", author)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "1ebdd6e66f7f44c485e9da5486812aad"
   },
   "source": [
    "## 3. Reading Sub-Links and Joining data\n",
    "\n",
    "In real world, you might need to gather information from multiple sources or at the very least multiple pages or urls from the same application to get the related information. Here is an example of how to do it using BeautifulSoup.\n",
    "\n",
    "We obtain the hrefs from anchor tags in two steps. The first step is to obtain the anchors by calling a .a on the base html doc. Then on the anchor element otain the href (link) by calling .get('href').\n",
    "\n",
    "```python\n",
    "    hrefs = soup.a\n",
    "    link = hrefs.get('href')\n",
    "    \n",
    "```\n",
    "\n",
    "The you could call the requests.get(link) to obtain the sub-link page and perform fresh parsing of the related content.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the quotes example, write a function get_uathor_link() and for each parent author class, pass the link and obtain the author brith-born-date and print them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "ac7c0fbb31af49b48f9519248e09eb02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mani/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/mani/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "#Modify the code below\n",
    "def get_author_dob(link_url):\n",
    "    response_auth = requests.get(link_url)\n",
    "##add your function code here\n",
    "    \n",
    "soup = BeautifulSoup(html)\n",
    "tag = soup.find_all(\"div\", class_=\"quote\")\n",
    "#print(tag)\n",
    "for t in tag:\n",
    "    text_out =  t.span.text \n",
    "## Add your logic to call the function get_author_dob() with links\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "cc25e284a9f84df589fefb77eab7ec16",
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "\n",
    "def get_author_dob(link_url):\n",
    "    response_auth = requests.get(link_url)\n",
    "    html_auth = response_auth.content\n",
    "    auth_soup = BeautifulSoup(html_auth)\n",
    "    auth_tag = auth_soup.find(\"span\", class_=\"author-born-date\")\n",
    "    return auth_tag.text\n",
    "\n",
    "output = []\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "tag = soup.find_all(\"div\", class_=\"quote\")\n",
    "#print(tag)\n",
    "for t in tag:\n",
    "    text_out =  t.span.text \n",
    "    print(t.span.text)\n",
    "    \n",
    "    a = t.find(\"small\", class_=\"author\")\n",
    "    author = a.text\n",
    "    text_out = text_out + ',\"' + author + '\"' \n",
    "    print(\"By :: \", author)\n",
    "    hrefs = t.a\n",
    "    link = hrefs.get('href')\n",
    "    link_url = url+ link\n",
    "    print(link_url)\n",
    "    dob = get_author_dob(link_url)\n",
    "    print(\"Author DOB:\", dob)\n",
    "    text_out = text_out + ',\"' + dob + '\"' \n",
    "    output.append(text_out)\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "4b35dc18504b40af8e177d7241c93387"
   },
   "source": [
    "## 4. Writing to file\n",
    "\n",
    "When we are done with the scraping operations we can write the data to a file.\n",
    "\n",
    "```python\n",
    "file_wr = open(\"output.csv\", \"a\")\n",
    "for line in output:\n",
    "    file_wr.write(str(line))\n",
    "file_wr.close()\n",
    "```\n",
    "\n",
    "### Exercise\n",
    "\n",
    "From the previous exercise with \"output\" from quotes, write all lines to a \"quotes.csv\" file by appending each line from scraped quotes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "55642aafe31a4c2c87ac3ace5cc87e5f"
   },
   "outputs": [],
   "source": [
    "#Write your code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "0f089f98dd5f4465ac6b67bbc94711cf",
    "collapsed": true,
    "hide_input": true
   },
   "source": [
    "### Solution\n",
    "\n",
    "```python\n",
    "file_wr = open(\"quotes.csv\", \"a\")\n",
    "for line in output:\n",
    "    file_wr.write(str(line) + '\\n')\n",
    "file_wr.close()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
