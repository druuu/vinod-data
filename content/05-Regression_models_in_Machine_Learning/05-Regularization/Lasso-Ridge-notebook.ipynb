{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l1"
    ]
   },
   "source": [
    "# Penalized Regression Models\n",
    "\n",
    "\n",
    "Under ideal conditions, the coefficients produced by ordinary least square regression are unbiased with very less variance. But in real conditions, Linear regression has low bias but suffers from high variance. We can decompose prediction error into squared bias and variance so it may be worth sacrificing some bias to achieve a lower variance. Whenever there is a collinearity between the predictor variables, the variance can become very large. Combatting collinearity by using biased models may result in regression models where the overall prediction error is competitive, meaning accuracy is better.\n",
    "\n",
    "Penalized models work by penalizing the magnitude of coefficients of features along with minimizing the error between predicted and actual observations. These are called ‘regularization’ techniques. There are two types of regularization techniques called the Ridge and Lasso.\n",
    "\n",
    "* **Ridge Regression:** Performs L2 regularization, i.e. adds penalty equivalent to square of the magnitude of coefficients \n",
    "\n",
    "        Minimization objective = Leat Square parameters + α * (sum of square of coefficients)\n",
    "\n",
    "* **Lasso Regression:** Performs L1 regularization, i.e. adds penalty equivalent to absolute value of the magnitude of coefficients \n",
    "\n",
    "        Minimization objective = Leat Square parameters + α * (sum of absolute value of coefficients)\n",
    "        \n",
    "Lets talk about them in more details.\n",
    "\n",
    "\n",
    "## Ridge Regression\n",
    "\n",
    "By standard defination, SSE which is the sum of squared errors is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "\\ SSE = \\sum\\limits_{i=1}^{n} {(y_i-\\hat{y_i})^2}\n",
    "\\end{align}\n",
    "\n",
    "When the model over-fits the data, or when there are issues with collinearity, the linear regression parameter estimates may become inflated as the square will increase the magnitude of the error even more. This will cause MSE to increase which is not wanted, so we may want to control the magnitude of these estimates to reduce the SSE. Ridge regression adds a penalty on the sum of the squared regression parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\ SSE_{L_2} = \\sum\\limits_{i=1}^{n} {(y_i-\\hat{y_i})^2} +  \\underbrace{\\lambda\\sum\\limits_{j=1}^{P} \\beta_j^2}_\\text{Penalty Term}\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"../images/ridge.png\", style=\"width: 700px;\">\n",
    "\n",
    "The “L2” signifies that a second-order penalty which is the square being used on the parameter estimates. In effect, this method shrinks the estimates towards 0 as the λ penalty becomes large (these techniques are sometimes called “shrinkage methods”).The effect of this penalty is that the parameter estimates are only allowed to become large if there is a proportional reduction in SSE. By adding the penalty, we are making a trade-off between the model variance and bias. By sacrificing some bias, we can often reduce the variance enough to make the overall accuracy better than unbiased models. Lets see how penalty effects the SSE:\n",
    "\n",
    "\n",
    "\n",
    "* β = 0:\n",
    "    * We’ll get the same coefficients as simple linear regression.\n",
    "* β = ∞:\n",
    "    * The coefficients will become zero because of infinite weightage on square of coefficients, anything less than zero will make the objective infinite.\n",
    "* 0 < β < ∞:\n",
    "    * The magnitude of β will decide the weightage given to different parts of objective.\n",
    "        The coefficients will be somewhere between 0 and ones for simple linear regression.\n",
    "        \n",
    "Lets utilize an example which uses penalized models to help us understand the differences. We will use Boston Data to see if our penalized models have any effect.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Implement a Linear Regression on Boston Housing Dataset with 5 fold cross validation. \n",
    "* Calculate the accuracy of the model and print it using the variable acc_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l1"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn import feature_selection, cross_validation, metrics\n",
    "from sklearn import datasets, linear_model\n",
    "from matplotlib import rcParams\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "boston_dataset = datasets.load_boston()\n",
    "boston_data = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "boston_data['MEDV'] = boston_dataset.target\n",
    "\n",
    "X = boston_data[['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']]\n",
    "y = boston_data['MEDV']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "use cross_val_score() to find the mean scores\n",
    "\n",
    "```python\n",
    "clf = linear_model.ElasticNet(.88)\n",
    "clf.fit(X, y)\n",
    "predicted_y = clf.predict(X)\n",
    "r2_score(y, predicted_y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l2"
    ]
   },
   "source": [
    "We get a score of 0.38 which is the accuracy. Lets now implement a Ridge regression to see if we get any better accuracy. We will use Ridge() function from sklearn linear_model to build this model.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Implement Ridge regression and find out the score. Put it in the variable called acc_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l2"
    ]
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "use ridge()\n",
    "\n",
    "```python\n",
    "clf = linear_model.Ridge()\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "acc_r=scores.mean()\n",
    "acc_r\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l3"
    ]
   },
   "source": [
    "We can clearly see that there has been an improvement in the scores. Hence, the ridge regression helps in increasing the accuracy by penalizing the model by introducing bias.\n",
    "This comes to our next question, How do we select the value of alpha parameter in the function?\n",
    "\n",
    "Lets see what effect does the value of penalty term have on the coffecients of the Ridge regression. Let take in different values of alpha and plot the ridge coefficient graph.\n",
    "\n",
    "```python\n",
    "n_alphas = 1000\n",
    "alphas = np.logspace(-5, 6, n_alphas)\n",
    "\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X, y)\n",
    "    coefs.append(ridge.coef_)\n",
    "```\n",
    "We have defined value of alpha from 10 to the power -5 to 10 to the power 6 using which we capture the values of all the coefficients in the regression. Now we plot and see the results.\n",
    "\n",
    "<img src=\"../images/ridge_coeff.png\", style=\"width: 700px;\"> \n",
    "\n",
    "When alpha is very large, the regularization effect dominates the squared loss function and the coefficients tend to zero. At the end of the path, as alpha tends toward zero and the solution tends towards the ordinary least squares, coefficients exhibit big oscillations. In practise it is necessary to tune alpha in such a way that a balance is maintained between both. \n",
    "\n",
    "Lets find the different value of accuracy when we vary alpha and plot it. \n",
    "\n",
    "\n",
    "### Exercise \n",
    "\n",
    "* Find different value of accuracy using for loop on the ridge function used earlier in the exercise and plot the graph\n",
    "* Use variable \"a\" to store the values of accuracy from the loop. Convert this variable into a dataframe called \"accuracy\". Use \"accuracy\" to plot the graph.\n",
    "* Find the value of Alpha for which the accuracy is the highest and store it in variable acc_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l3"
    ]
   },
   "outputs": [],
   "source": [
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "Use idxmax() to find the index at which the values is highest\n",
    "\n",
    "```python\n",
    "a=[]\n",
    "for alpha in range(1,500):\n",
    "    clf = linear_model.Ridge(alpha)\n",
    "    scores =cross_val_score(clf, X, y, cv = 5)\n",
    "    a.append(scores.mean())\n",
    "accuracy=pd.DataFrame(a)\n",
    "accuracy.plot()\n",
    "plt.show()\n",
    "acc_max=accuracy.idxmax()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l4"
    ]
   },
   "source": [
    "We can see from above graph that accuracy increases till some value and then starts decreasing as the value of λ increases.\n",
    "\n",
    "## Lasso Regression\n",
    "\n",
    "Lasso regression also known as Least Absolute Shrinkage and Selection Operator is another penalized model which is used for variable selection and regularization in order to enhance the prediction accuracy. This model uses a similar penalty to ridge regression:\n",
    "\n",
    "\\begin{align}\n",
    "\\ SSE_{L_1} = \\sum\\limits_{i=1}^{n} {(y_i-\\hat{y_i})^2} +  \\underbrace{\\lambda\\sum\\limits_{j=1}^{P} \\lvert  \\beta_j\\rvert}_\\text{Penalty Term}\n",
    "\\end{align}\n",
    "\n",
    "<img src=\"../images/lasso.png\", style=\"width: 700px;\">\n",
    "\n",
    "While this may seem like a small modification, the practical implications are significant. While the regression coefficients are still shrunk towards 0, a consequence of penalizing the absolute values is that some parameters are actually set to 0 for some value of λ. Thus the lasso yields models that simultaneously use regularization to improve the model and to conduct feature selection. While ridge regression shrinks the parameter estimates towards 0, the model does not set the values to absolute 0 for any value of the penalty.\n",
    "\n",
    "In comparing these two types of penalty, Ridge regression is known to shrink the coefficients of correlated predictors\n",
    "towards each other, allowing them to borrow strength from each other, lasso, on the other hand, is somewhat indifferent to very correlated predictors, and will tend to pick one and ignore the rest. \n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Implement a LASSO Regression on Boston Housing Dataset with 5 fold cross validation. \n",
    "* Calculate the accuracy of the model and print it using the variable acc_lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l4"
    ]
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "use Lasso()\n",
    "\n",
    "```python\n",
    "clf = linear_model.Lasso()\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "acc_lasso=scores.mean()\n",
    "acc_lasso\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "l5"
    ]
   },
   "source": [
    "We can see that the accuracy which we have got is still better than that from simple OLS method. Similar to Ridge regression, we can see the Regression Coefficients Progression for Lasso Paths for various value of Alpha,\n",
    "\n",
    "<img src=\"../images/lasso_coeff.png\", style=\"width: 700px;\"> \n",
    "\n",
    "\n",
    "## Elastic Nets\n",
    "\n",
    "The elastic net is a regularized regression method that linearly combines the L1 and L2 penalties of the lasso and ridge methods.This model combines the two types of penalties as:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "\\ SSE_{E_{net}} = \\sum\\limits_{i=1}^{n} {(y_i-\\hat{y_i})^2} +  \\underbrace{\\lambda_1\\sum\\limits_{j=1}^{P} \\beta_j^2 + \\lambda_2\\sum\\limits_{j=1}^{P} \\lvert  \\beta_j\\rvert}_\\text{Penalty Term}\n",
    "\\end{align}\n",
    "\n",
    "The advantage of this model is that it enables effective regularization via the ridge-type penalty with the feature selection quality of the lasso. Hence Elastic Net enables us to take in advantages from both LASSO and RIdge regularization, combine it and use it in its algorithm.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "* Implement a Elastic Net regularization on Boston Housing Dataset with 5 fold cross validation using function ElasticNet(). \n",
    "* Calculate the accuracy of the model and print it using the variable acc_enet\n",
    "* Print and compare the three variables acc_lr, acc_r, acc_lasso and acc_enet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true,
    "tags": [
     "l5"
    ]
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "## Solution\n",
    "use ElasticNet()\n",
    "\n",
    "```python\n",
    "clf = linear_model.ElasticNet()\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "acc_enet=scores.mean()\n",
    "print(\"Linear Regression Accuracy : \",acc_lr,\"\\n Ridge Regression Accuracy : \",acc_r,\"\\n Lasso Regression Accuracy : \",acc_lasso,\"\\n Elastic Net accuracy : \",acc_enet)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
